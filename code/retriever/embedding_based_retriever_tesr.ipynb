{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "import pickle\n",
        "import networkx as nx\n",
        "import ast\n",
        "import re\n",
        "import json\n",
        "import chromadb\n",
        "import cProfile\n",
        "import pstats\n",
        "\n",
        "from datasets import load_dataset\n",
        "from pprint import pprint\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
        "from langchain.agents import tool\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.callbacks import get_openai_callback\n",
        "\n",
        "from commit import update\n",
        "from utils.utils import serialize_dict_to_json, deserialize_json_to_dict\n",
        "from utils.chunk import SimpleFixedLengthChunker\n",
        "from utils.compress import get_skeleton\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "sflc = SimpleFixedLengthChunker()\n",
        "\n",
        "dataset = load_dataset(\"lahirum/SWE_Experimental\", split=\"train\")\n",
        "# filter = [0, 1, 2, 3, 4,5, 6, 7, 8, 9]\n",
        "# dataset = dataset.select(filter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a4fa7cd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_graph(pickle_path):\n",
        "    \"\"\"Loads a NetworkX DiGraph from a pickle file.\"\"\"\n",
        "    with open(pickle_path, \"rb\") as f:\n",
        "        graph = pickle.load(f)\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import prompts\n",
        "import schema\n",
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "# from langchain_deepseek import ChatDeepSeek\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.1,\n",
        "    #max_retries=2,\n",
        ")\n",
        "\n",
        "# llm_deepseek = ChatDeepSeek(\n",
        "#     model=\"deepseek-reasoner\",\n",
        "#     temperature=0,\n",
        "#     max_tokens=None,\n",
        "#     timeout=None,\n",
        "#     max_retries=2,\n",
        "#     # other params...\n",
        "# )\n",
        "\n",
        "llm_large = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0,\n",
        "    max_retries=2,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1211925/3732032862.py:4: LangChainDeprecationWarning: The function `_convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 1.0. Use :meth:`~langchain_core.utils.function_calling.convert_to_openai_function()` instead.\n",
            "  functions=[convert_pydantic_to_openai_function(schema.SuspiciousComponentOutput)],\n"
          ]
        }
      ],
      "source": [
        "parser = JsonOutputFunctionsParser()\n",
        "\n",
        "model_extract = llm_large.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousComponentOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "extract_chain = prompts.prompt_extract | model_extract\n",
        "\n",
        "model_select = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.FileSuspicionOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_chain = prompts.file_path_filter_prompt | model_select\n",
        "\n",
        "model_filter_list = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousFilesOutputList)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "filter_list_chain = prompts.get_suspicious_file_list_from_list_of_files_prompt | model_filter_list \n",
        "\n",
        "model_select_list = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousFilesOutputList)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_list_chain = prompts.suspicious_files_filter_list_usingclfn_prompt | model_select_list\n",
        "\n",
        "model_select_with_reason = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousFileReasoningOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_with_reason_chain = prompts.suspicious_files_with_reason_prompt | model_select_with_reason\n",
        "\n",
        "model_select_directory = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousDirectoryOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_directory_chain = prompts.suspicious_directory_prompt | model_select_directory\n",
        "\n",
        "generate_multiple_descriptions = prompts.prompt_embedding_retriver | llm\n",
        "\n",
        "# deep_reasoning_chain = prompts.deep_reasoning_prompt | llm_deepseek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def start(inputs):\n",
        "    problem_description = inputs['problem_description']\n",
        "    name = inputs['name']\n",
        "    graph = inputs['graph'] \n",
        "    commit_id = inputs['commit_id']\n",
        "    graph = inputs['graph'] \n",
        "\n",
        "    update(name, commit_id)\n",
        "\n",
        "    with get_openai_callback() as callback:\n",
        "        result = extract_chain.invoke({\"problem_description\": problem_description})\n",
        "        print(callback)\n",
        "\n",
        "    result = result.additional_kwargs['function_call']['arguments']\n",
        "    result = json.loads(result)\n",
        "    result['name'] = name\n",
        "    result['problem_description'] = problem_description\n",
        "    result['graph'] = graph\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def embedding_retriever(inputs):\n",
        "    problem_description = inputs['problem_description']\n",
        "    name = inputs['name']\n",
        "    \n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "    chroma_client = chromadb.PersistentClient(f\"chroma_db\")\n",
        "    collection = chroma_client.get_collection(name=f\"{name}_chroma_index\")\n",
        "\n",
        "    vector_store = Chroma(\n",
        "        client=chroma_client,\n",
        "        collection_name=f\"{name}_chroma_index\",\n",
        "        embedding_function=embeddings,\n",
        "    )\n",
        "\n",
        "    results = vector_store.similarity_search(problem_description, k=10,)\n",
        "    # results = vector_store.max_marginal_relevance_search(problem_description, k=10, lambda_mult=0.5)\n",
        "\n",
        "    file = deserialize_json_to_dict(\"django_file_ids.json\")\n",
        "    structure = {}\n",
        "    for result in results:\n",
        "        file_ids = file[result.metadata[\"filename\"]].split(\":\")\n",
        "        chunk_docs_of_file = vector_store.get_by_ids(file_ids)\n",
        "        structure[result.metadata['filename']] = get_skeleton(sflc.dechunk_docs(chunk_docs_of_file))\n",
        "        \n",
        "    with get_openai_callback() as callback:\n",
        "        answer = select_with_reason_chain.invoke({\"problem_description\": problem_description, \"file_structure\": structure})\n",
        "        print(callback)\n",
        "    answer = json.loads(answer.additional_kwargs['function_call']['arguments'])\n",
        "\n",
        "    return answer['suspicious_files']\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "django\n",
            "DiGraph with 25367 nodes and 32407 edges\n",
            "Node 'django/django/conf/locale/__init__.py' does not exist.\n",
            "Node 'django/django/conf/locale/ar_DZ/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/az/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/cs/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/cy/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/de/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/de_CH/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/el/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/en/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/en_AU/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/en_GB/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/eo/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/fi/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/fr/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/hr/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/hu/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/id/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/ig/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/it/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/ka/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/ko/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/ky/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/lt/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/lv/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/mk/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/ml/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/nb/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/nl/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/nn/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/pl/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/pt/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/pt_BR/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/ru/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/sk/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/sl/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/sr/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/sr_Latn/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/sv/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/tg/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/tk/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/tr/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/uk/formats.py' does not exist.\n",
            "Node 'django/django/conf/locale/uz/formats.py' does not exist.\n",
            "Node 'django/django/contrib/gis/db/backends/postgis/const.py' does not exist.\n",
            "Node 'django/django/contrib/sitemaps/management/__init__.py' does not exist.\n",
            "Node 'django/django/contrib/sitemaps/management/commands/__init__.py' does not exist.\n",
            "Node 'django/django/test/__init__.py' does not exist.\n",
            "Node 'django/setup.py' does not exist.\n",
            "Node 'django/tests/apps/explicit_default_config_app/__init__.py' does not exist.\n",
            "Node 'django/tests/apps/explicit_default_config_empty_apps/apps.py' does not exist.\n",
            "Node 'django/tests/apps/explicit_default_config_mismatch_app/__init__.py' does not exist.\n",
            "Node 'django/tests/auth_tests/models/__init__.py' does not exist.\n",
            "Node 'django/tests/auth_tests/settings.py' does not exist.\n",
            "Node 'django/tests/builtin_server/urls.py' does not exist.\n",
            "Node 'django/tests/check_framework/urls/path_compatibility/contains_re_named_group.py' does not exist.\n",
            "Node 'django/tests/file_uploads/urls.py' does not exist.\n",
            "Node 'django/tests/flatpages_tests/absolute_urls.py' does not exist.\n",
            "Node 'django/tests/flatpages_tests/no_slash_urls.py' does not exist.\n",
            "Node 'django/tests/flatpages_tests/urls.py' does not exist.\n",
            "Node 'django/tests/generic_views/urls.py' does not exist.\n",
            "Node 'django/tests/gis_tests/geoapp/urls.py' does not exist.\n",
            "Node 'django/tests/handlers/urls.py' does not exist.\n",
            "Node 'django/tests/i18n/patterns/urls/default.py' does not exist.\n",
            "Node 'django/tests/middleware/urls.py' does not exist.\n",
            "Node 'django/tests/middleware_exceptions/urls.py' does not exist.\n",
            "Node 'django/tests/model_meta/results.py' does not exist.\n",
            "Node 'django/tests/staticfiles_tests/settings.py' does not exist.\n",
            "Node 'django/tests/syndication_tests/urls.py' does not exist.\n",
            "Node 'django/tests/test_client/urls.py' does not exist.\n",
            "Node 'django/tests/test_client_regress/urls.py' does not exist.\n",
            "Node 'django/tests/test_sqlite.py' does not exist.\n",
            "Node 'django/tests/urlpatterns/path_same_name_urls.py' does not exist.\n",
            "Node 'django/tests/urlpatterns/path_urls.py' does not exist.\n",
            "Node 'django/tests/view_tests/urls.py' does not exist.\n",
            "Checked out to df46b329e0900e9e4dc1d60816c1dce6dfc1094e\n",
            "DiGraph with 23394 nodes and 29690 edges\n",
            "Tokens Used: 1542\n",
            "\tPrompt Tokens: 1511\n",
            "\t\tPrompt Tokens Cached: 0\n",
            "\tCompletion Tokens: 31\n",
            "\t\tReasoning Tokens: 0\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.0040875\n",
            "Tokens Used: 2403\n",
            "\tPrompt Tokens: 2283\n",
            "\t\tPrompt Tokens Cached: 0\n",
            "\tCompletion Tokens: 120\n",
            "\t\tReasoning Tokens: 0\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.0004144499999999999\n"
          ]
        }
      ],
      "source": [
        "retrivals = []\n",
        "\n",
        "for i in range(3, 4):\n",
        "    ips = {}\n",
        "\n",
        "    ips[\"commit_id\"] = dataset[i]['base_commit']\n",
        "    ips[\"name\"]= dataset[i]['instance_id'].split(\"__\")[0]\n",
        "    ips[\"problem_description\"] = dataset[i]['problem_statement']\n",
        "    ips[\"graph\"] = load_graph(f\"graph_{ips['name']}.pkl\")\n",
        "\n",
        "    answer = generate_multiple_descriptions.invoke({\"problem_description\": ips[\"problem_description\"]})\n",
        "\n",
        "    ips[\"problem_description\"] = \\\n",
        "        f\"\"\"## **Original GitHub issue description**:\\n\\n{ips[\"problem_description\"]}\\n\\n\\n## **Generated descriptions**:\\n\\n{answer.content}\"\"\"\n",
        "\n",
        "    profiler = cProfile.Profile()\n",
        "\n",
        "    profiler.enable()  # Start profiling\n",
        "\n",
        "    start(ips)\n",
        "    ans = embedding_retriever(ips)\n",
        "\n",
        "    profiler.disable() # Stop profiling\n",
        "    profiler.dump_stats('profiling_results.prof')  # Save results to a file\n",
        "\n",
        "    temp_dict = {dataset[i]['instance_id']: {}}\n",
        "    for level, a in enumerate(ans):\n",
        "        temp_dict[dataset[i]['instance_id']][level] = {\n",
        "            \"identified_file\": a['file'],\n",
        "            \"erroneous_file\": dataset[i][\"erroneous_file\"].strip(),\n",
        "        }\n",
        "    retrivals.append(temp_dict)\n",
        "    serialize_dict_to_json(retrivals, \"embedding_based_retrievals.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9d78cca4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Jun  1 18:57:58 2025    profiling_results.prof\n",
            "\n",
            "         66087612 function calls (65596861 primitive calls) in 683.986 seconds\n",
            "\n",
            "   Ordered by: cumulative time\n",
            "   List reduced from 2684 to 30 due to restriction <30>\n",
            "\n",
            "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
            "        1    0.011    0.011  672.553  672.553 /tmp/ipykernel_1211925/2347640675.py:1(start)\n",
            "        1    0.153    0.153  670.007  670.007 /home/e19163/FYP/retriver_test/e19-4yp-Solve-Issues-In-Large-Code-Repositories/code/retriever/commit.py:149(update)\n",
            "        3    0.501    0.167  399.979  133.326 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/langchain_core/vectorstores/base.py:262(add_documents)\n",
            "        3    0.016    0.005  399.469  133.156 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:497(add_texts)\n",
            "        3    0.000    0.000  298.050   99.350 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/chromadb/api/models/Collection.py:340(upsert)\n",
            "        3    0.002    0.001  285.747   95.249 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/chromadb/api/rust.py:459(_upsert)\n",
            "        3  285.744   95.248  285.744   95.248 {method 'upsert' of 'builtins.Bindings' objects}\n",
            "        3    0.000    0.000  164.517   54.839 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:1247(delete)\n",
            "        3    0.000    0.000  164.517   54.839 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/chromadb/api/models/Collection.py:385(delete)\n",
            "        3    0.002    0.001  164.511   54.837 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/chromadb/api/rust.py:535(_delete)\n",
            "        3  164.508   54.836  164.508   54.836 {method 'delete' of 'builtins.Bindings' objects}\n",
            "       14    0.000    0.000  109.394    7.814 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/openai/_base_client.py:1225(post)\n",
            "       14    0.001    0.000  109.390    7.814 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/openai/_base_client.py:931(request)\n",
            "       14    0.000    0.000  106.830    7.631 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/httpx/_client.py:879(send)\n",
            "        4    0.015    0.004  103.216   25.804 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:557(embed_documents)\n",
            "        4    0.089    0.022  103.201   25.800 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:449(_get_len_safe_embeddings)\n",
            "       12    0.001    0.000  101.694    8.475 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/openai/resources/embeddings.py:47(create)\n",
            "     5431    0.067    0.000   99.856    0.018 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/httpcore/_sync/http11.py:209(_receive_event)\n",
            "     5408    0.039    0.000   99.273    0.018 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/httpcore/_backends/sync.py:124(read)\n",
            "     5408    0.013    0.000   99.175    0.018 /home/e19163/miniconda/envs/fyp/lib/python3.11/ssl.py:1288(recv)\n",
            "     5408    0.012    0.000   99.160    0.018 /home/e19163/miniconda/envs/fyp/lib/python3.11/ssl.py:1157(read)\n",
            "     5408   99.147    0.018   99.147    0.018 {method 'read' of '_ssl._SSLSocket' objects}\n",
            "        3    0.000    0.000   64.879   21.626 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/git/cmd.py:1523(_call_process)\n",
            "        3    0.000    0.000   64.878   21.626 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/git/cmd.py:1079(execute)\n",
            "        2    0.000    0.000   64.874   32.437 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/git/cmd.py:986(<lambda>)\n",
            "        1    0.000    0.000   61.804   61.804 /home/e19163/FYP/retriver_test/e19-4yp-Solve-Issues-In-Large-Code-Repositories/code/retriever/commit.py:118(checkout_commit)\n",
            "       14    0.000    0.000   61.009    4.358 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/httpx/_client.py:930(_send_handling_auth)\n",
            "       14    0.000    0.000   61.008    4.358 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/httpx/_client.py:964(_send_handling_redirects)\n",
            "       14    0.001    0.000   61.008    4.358 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/httpx/_client.py:1001(_send_single_request)\n",
            "       14    0.001    0.000   60.991    4.356 /home/e19163/miniconda/envs/fyp/lib/python3.11/site-packages/httpx/_transports/default.py:230(handle_request)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<pstats.Stats at 0x7e3dbb351a10>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p = pstats.Stats('profiling_results.prof')\n",
        "p.sort_stats('cumulative').print_stats(30)  # Sort by cumulative time and print top 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e1130f7",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
