{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "import pickle\n",
        "import networkx as nx\n",
        "import ast\n",
        "import re\n",
        "import json\n",
        "import chromadb\n",
        "\n",
        "from datasets import load_dataset\n",
        "from pprint import pprint\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
        "from langchain.agents import tool\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.callbacks import get_openai_callback\n",
        "\n",
        "from commit import update\n",
        "from utils.utils import serialize_dict_to_json, deserialize_json_to_dict\n",
        "from utils.chunk import SimpleFixedLengthChunker\n",
        "from utils.compress import get_skeleton\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "sflc = SimpleFixedLengthChunker()\n",
        "\n",
        "dataset = load_dataset(\"lahirum/SWE_Experimental\", split=\"train\")\n",
        "# filter = [0, 1, 2, 3, 4,5, 6, 7, 8, 9]\n",
        "# dataset = dataset.select(filter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4fa7cd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_graph(pickle_path):\n",
        "    \"\"\"Loads a NetworkX DiGraph from a pickle file.\"\"\"\n",
        "    with open(pickle_path, \"rb\") as f:\n",
        "        graph = pickle.load(f)\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import prompts\n",
        "import schema\n",
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "# from langchain_deepseek import ChatDeepSeek\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.1,\n",
        "    #max_retries=2,\n",
        ")\n",
        "\n",
        "# llm_deepseek = ChatDeepSeek(\n",
        "#     model=\"deepseek-reasoner\",\n",
        "#     temperature=0,\n",
        "#     max_tokens=None,\n",
        "#     timeout=None,\n",
        "#     max_retries=2,\n",
        "#     # other params...\n",
        "# )\n",
        "\n",
        "llm_large = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0,\n",
        "    max_retries=2,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1945654/3732032862.py:4: LangChainDeprecationWarning: The function `_convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 1.0. Use :meth:`~langchain_core.utils.function_calling.convert_to_openai_function()` instead.\n",
            "  functions=[convert_pydantic_to_openai_function(schema.SuspiciousComponentOutput)],\n"
          ]
        }
      ],
      "source": [
        "parser = JsonOutputFunctionsParser()\n",
        "\n",
        "model_extract = llm_large.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousComponentOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "extract_chain = prompts.prompt_extract | model_extract\n",
        "\n",
        "model_select = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.FileSuspicionOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_chain = prompts.file_path_filter_prompt | model_select\n",
        "\n",
        "model_filter_list = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousFilesOutputList)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "filter_list_chain = prompts.get_suspicious_file_list_from_list_of_files_prompt | model_filter_list \n",
        "\n",
        "model_select_list = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousFilesOutputList)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_list_chain = prompts.suspicious_files_filter_list_usingclfn_prompt | model_select_list\n",
        "\n",
        "model_select_with_reason = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousFileReasoningOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_with_reason_chain = prompts.suspicious_files_with_reason_prompt | model_select_with_reason\n",
        "\n",
        "model_select_directory = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousDirectoryOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_directory_chain = prompts.suspicious_directory_prompt | model_select_directory\n",
        "\n",
        "generate_multiple_descriptions = prompts.prompt_embedding_retriver | llm\n",
        "\n",
        "# deep_reasoning_chain = prompts.deep_reasoning_prompt | llm_deepseek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def start(inputs):\n",
        "    problem_description = inputs['problem_description']\n",
        "    name = inputs['name']\n",
        "    graph = inputs['graph'] \n",
        "    commit_id = inputs['commit_id']\n",
        "    graph = inputs['graph'] \n",
        "\n",
        "    update(name, commit_id)\n",
        "\n",
        "    with get_openai_callback() as callback:\n",
        "        result = extract_chain.invoke({\"problem_description\": problem_description})\n",
        "        print(callback)\n",
        "\n",
        "    result = result.additional_kwargs['function_call']['arguments']\n",
        "    result = json.loads(result)\n",
        "    result['name'] = name\n",
        "    result['problem_description'] = problem_description\n",
        "    result['graph'] = graph\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def embedding_retriever(inputs):\n",
        "    problem_description = inputs['problem_description']\n",
        "    name = inputs['name']\n",
        "    \n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "    chroma_client = chromadb.PersistentClient(f\"chroma_db\")\n",
        "    collection = chroma_client.get_collection(name=f\"{name}_chroma_index\")\n",
        "\n",
        "    vector_store = Chroma(\n",
        "        client=chroma_client,\n",
        "        collection_name=f\"{name}_chroma_index\",\n",
        "        embedding_function=embeddings,\n",
        "    )\n",
        "\n",
        "    results = vector_store.similarity_search(problem_description, k=10,)\n",
        "    # results = vector_store.max_marginal_relevance_search(problem_description, k=10, lambda_mult=0.5)\n",
        "\n",
        "    file = deserialize_json_to_dict(\"django_file_ids.json\")\n",
        "    structure = {}\n",
        "    for result in results:\n",
        "        file_ids = file[result.metadata[\"filename\"]].split(\":\")\n",
        "        chunk_docs_of_file = vector_store.get_by_ids(file_ids)\n",
        "        structure[result.metadata['filename']] = get_skeleton(sflc.dechunk_docs(chunk_docs_of_file))\n",
        "        \n",
        "    with get_openai_callback() as callback:\n",
        "        answer = select_with_reason_chain.invoke({\"problem_description\": problem_description, \"file_structure\": structure})\n",
        "        print(callback)\n",
        "    answer = json.loads(answer.additional_kwargs['function_call']['arguments'])\n",
        "\n",
        "    return answer['suspicious_files']\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "django\n",
            "DiGraph with 28091 nodes and 36461 edges\n",
            "Checked out to 4fd3044ca0135da903a70dfb66992293f529ecf1\n",
            "DiGraph with 28091 nodes and 36461 edges\n",
            "Tokens Used: 1216\n",
            "\tPrompt Tokens: 1189\n",
            "\t\tPrompt Tokens Cached: 0\n",
            "\tCompletion Tokens: 27\n",
            "\t\tReasoning Tokens: 0\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.0032425\n",
            "Tokens Used: 14704\n",
            "\tPrompt Tokens: 14489\n",
            "\t\tPrompt Tokens Cached: 0\n",
            "\tCompletion Tokens: 215\n",
            "\t\tReasoning Tokens: 0\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.00230235\n",
            "[{'erroneous_file': 'django/core/validators.py',\n",
            "  'identified_file': 'django/django/forms/fields.py',\n",
            "  'instance_id': 'django__django-15202',\n",
            "  'level': 0},\n",
            " {'erroneous_file': 'django/core/validators.py',\n",
            "  'identified_file': 'django/django/core/validators.py',\n",
            "  'instance_id': 'django__django-15202',\n",
            "  'level': 1}]\n",
            "django\n",
            "DiGraph with 28091 nodes and 36461 edges\n",
            "Checked out to 4a72da71001f154ea60906a2f74898d32b7322a7\n",
            "DiGraph with 29322 nodes and 38069 edges\n",
            "Tokens Used: 1939\n",
            "\tPrompt Tokens: 1912\n",
            "\t\tPrompt Tokens Cached: 0\n",
            "\tCompletion Tokens: 27\n",
            "\t\tReasoning Tokens: 0\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.00505\n",
            "Tokens Used: 13836\n",
            "\tPrompt Tokens: 13601\n",
            "\t\tPrompt Tokens Cached: 0\n",
            "\tCompletion Tokens: 235\n",
            "\t\tReasoning Tokens: 0\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.00218115\n",
            "[{'erroneous_file': 'django/core/validators.py',\n",
            "  'identified_file': 'django/django/forms/fields.py',\n",
            "  'instance_id': 'django__django-15202',\n",
            "  'level': 0},\n",
            " {'erroneous_file': 'django/core/validators.py',\n",
            "  'identified_file': 'django/django/core/validators.py',\n",
            "  'instance_id': 'django__django-15202',\n",
            "  'level': 1},\n",
            " {'erroneous_file': 'django/db/migrations/serializer.py',\n",
            "  'identified_file': 'django/django/db/migrations/autodetector.py',\n",
            "  'instance_id': 'django__django-17087',\n",
            "  'level': 0},\n",
            " {'erroneous_file': 'django/db/migrations/serializer.py',\n",
            "  'identified_file': 'django/django/db/models/fields/__init__.py',\n",
            "  'instance_id': 'django__django-17087',\n",
            "  'level': 1},\n",
            " {'erroneous_file': 'django/db/migrations/serializer.py',\n",
            "  'identified_file': 'django/django/db/migrations/questioner.py',\n",
            "  'instance_id': 'django__django-17087',\n",
            "  'level': 2}]\n"
          ]
        }
      ],
      "source": [
        "retrivals = []\n",
        "\n",
        "for i in range(0, 2):\n",
        "    ips = {}\n",
        "\n",
        "    ips[\"commit_id\"] = dataset[i]['base_commit']\n",
        "    ips[\"name\"]= dataset[i]['instance_id'].split(\"__\")[0]\n",
        "    ips[\"problem_description\"] = dataset[i]['problem_statement']\n",
        "    ips[\"graph\"] = load_graph(f\"graph_{ips['name']}.pkl\")\n",
        "\n",
        "    answer = generate_multiple_descriptions.invoke({\"problem_description\": ips[\"problem_description\"]})\n",
        "\n",
        "    ips[\"problem_description\"] = \\\n",
        "        f\"\"\"## **Original GitHub issue description**:\\n\\n{ips[\"problem_description\"]}\\n\\n\\n## **Generated descriptions**:\\n\\n{answer.content}\"\"\"\n",
        "\n",
        "    start(ips)\n",
        "    ans = embedding_retriever(ips)\n",
        "\n",
        "    for level, a in enumerate(ans):\n",
        "        retrivals.append({\n",
        "            \"instance_id\": dataset[i]['instance_id'],\n",
        "            \"level\": level,\n",
        "            \"identified_file\": a['file'],\n",
        "            \"erroneous_file\": dataset[i][\"erroneous_file\"].strip(),\n",
        "        })\n",
        "    pprint(retrivals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "66577cc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.utils import serialize_dict_to_json\n",
        "serialize_dict_to_json(retrivals, \"./embedding_baseed_retrivals.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47e67331",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
