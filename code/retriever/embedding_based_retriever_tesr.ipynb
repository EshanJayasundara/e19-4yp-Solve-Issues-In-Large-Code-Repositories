{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "import pickle\n",
        "import networkx as nx\n",
        "import ast\n",
        "import re\n",
        "import json\n",
        "import chromadb\n",
        "\n",
        "from datasets import load_dataset\n",
        "from pprint import pprint\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
        "from langchain.agents import tool\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.callbacks import get_openai_callback\n",
        "\n",
        "from commit import update\n",
        "from utils.utils import serialize_dict_to_json, deserialize_json_to_dict\n",
        "from utils.chunk import SimpleFixedLengthChunker\n",
        "from utils.compress import get_skeleton\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "sflc = SimpleFixedLengthChunker()\n",
        "\n",
        "dataset = load_dataset(\"lahirum/SWE_Experimental\", split=\"train\")\n",
        "# filter = [0, 1, 2, 3, 4,5, 6, 7, 8, 9]\n",
        "# dataset = dataset.select(filter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4fa7cd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_graph(pickle_path):\n",
        "    \"\"\"Loads a NetworkX DiGraph from a pickle file.\"\"\"\n",
        "    with open(pickle_path, \"rb\") as f:\n",
        "        graph = pickle.load(f)\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import prompts\n",
        "import schema\n",
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "# from langchain_deepseek import ChatDeepSeek\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.1,\n",
        "    #max_retries=2,\n",
        ")\n",
        "\n",
        "# llm_deepseek = ChatDeepSeek(\n",
        "#     model=\"deepseek-reasoner\",\n",
        "#     temperature=0,\n",
        "#     max_tokens=None,\n",
        "#     timeout=None,\n",
        "#     max_retries=2,\n",
        "#     # other params...\n",
        "# )\n",
        "\n",
        "llm_large = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0,\n",
        "    max_retries=2,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parser = JsonOutputFunctionsParser()\n",
        "\n",
        "model_extract = llm_large.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousComponentOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "extract_chain = prompts.prompt_extract | model_extract\n",
        "\n",
        "model_select = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.FileSuspicionOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_chain = prompts.file_path_filter_prompt | model_select\n",
        "\n",
        "model_filter_list = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousFilesOutputList)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "filter_list_chain = prompts.get_suspicious_file_list_from_list_of_files_prompt | model_filter_list \n",
        "\n",
        "model_select_list = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousFilesOutputList)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_list_chain = prompts.suspicious_files_filter_list_usingclfn_prompt | model_select_list\n",
        "\n",
        "model_select_with_reason = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousFileReasoningOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_with_reason_chain = prompts.suspicious_files_with_reason_prompt | model_select_with_reason\n",
        "\n",
        "model_select_directory = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousDirectoryOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_directory_chain = prompts.suspicious_directory_prompt | model_select_directory\n",
        "\n",
        "generate_multiple_descriptions = prompts.prompt_embedding_retriver | llm\n",
        "\n",
        "# deep_reasoning_chain = prompts.deep_reasoning_prompt | llm_deepseek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def start(inputs):\n",
        "    problem_description = inputs['problem_description']\n",
        "    name = inputs['name']\n",
        "    graph = inputs['graph'] \n",
        "    commit_id = inputs['commit_id']\n",
        "    graph = inputs['graph'] \n",
        "\n",
        "    update(name, commit_id)\n",
        "\n",
        "    with get_openai_callback() as callback:\n",
        "        result = extract_chain.invoke({\"problem_description\": problem_description})\n",
        "        print(callback)\n",
        "\n",
        "    result = result.additional_kwargs['function_call']['arguments']\n",
        "    result = json.loads(result)\n",
        "    result['name'] = name\n",
        "    result['problem_description'] = problem_description\n",
        "    result['graph'] = graph\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def embedding_retriever(inputs):\n",
        "    problem_description = inputs['problem_description']\n",
        "    name = inputs['name']\n",
        "    \n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "    chroma_client = chromadb.PersistentClient(f\"chroma_db\")\n",
        "    collection = chroma_client.get_collection(name=f\"{name}_chroma_index\")\n",
        "\n",
        "    vector_store = Chroma(\n",
        "        client=chroma_client,\n",
        "        collection_name=f\"{name}_chroma_index\",\n",
        "        embedding_function=embeddings,\n",
        "    )\n",
        "\n",
        "    results = vector_store.similarity_search(problem_description, k=10,)\n",
        "    # results = vector_store.max_marginal_relevance_search(problem_description, k=10, lambda_mult=0.5)\n",
        "\n",
        "    file = deserialize_json_to_dict(\"django_file_ids.json\")\n",
        "    structure = {}\n",
        "    for result in results:\n",
        "        file_ids = file[result.metadata[\"filename\"]].split(\":\")\n",
        "        chunk_docs_of_file = vector_store.get_by_ids(file_ids)\n",
        "        structure[result.metadata['filename']] = get_skeleton(sflc.dechunk_docs(chunk_docs_of_file))\n",
        "        \n",
        "    with get_openai_callback() as callback:\n",
        "        answer = select_with_reason_chain.invoke({\"problem_description\": problem_description, \"file_structure\": structure})\n",
        "        print(callback)\n",
        "    answer = json.loads(answer.additional_kwargs['function_call']['arguments'])\n",
        "\n",
        "    return answer['suspicious_files']\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "retrivals = []\n",
        "\n",
        "for i in range(0, 20):\n",
        "    ips = {}\n",
        "\n",
        "    ips[\"commit_id\"] = dataset[i]['base_commit']\n",
        "    ips[\"name\"]= dataset[i]['instance_id'].split(\"__\")[0]\n",
        "    ips[\"problem_description\"] = dataset[i]['problem_statement']\n",
        "    ips[\"graph\"] = load_graph(f\"graph_{ips['name']}.pkl\")\n",
        "\n",
        "    answer = generate_multiple_descriptions.invoke({\"problem_description\": ips[\"problem_description\"]})\n",
        "\n",
        "    ips[\"problem_description\"] = \\\n",
        "        f\"\"\"## **Original GitHub issue description**:\\n\\n{ips[\"problem_description\"]}\\n\\n\\n## **Generated descriptions**:\\n\\n{answer.content}\"\"\"\n",
        "\n",
        "    start(ips)\n",
        "    ans = embedding_retriever(ips)\n",
        "\n",
        "    for level, a in enumerate(ans):\n",
        "        retrivals.append({\n",
        "            \"instance_id\": dataset[i]['instance_id'],\n",
        "            \"level\": level,\n",
        "            \"identified_file\": a['file'],\n",
        "            \"erroneous_file\": dataset[i][\"erroneous_file\"].strip(),\n",
        "        })\n",
        "        \n",
        "    serialize_dict_to_json(retrivals, \"embedding_based_retrievals.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d78cca4",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
