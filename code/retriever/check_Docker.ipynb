{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sandbox.specs import (\n",
    "    MAP_REPO_TO_REQS_PATHS,\n",
    "    MAP_REPO_VERSION_TO_SPECS_PY,\n",
    ")\n",
    "from sandbox import bash_session, docker_build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lahiru-menik/miniconda3/envs/agentless/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lahirum/SWE_Experimental\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': '3.9',\n",
       " 'packages': 'requirements.txt',\n",
       " 'install': 'python -m pip install -e .',\n",
       " 'test_cmd': './tests/runtests.py --verbosity 2'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAP_REPO_VERSION_TO_SPECS_PY[dataset[0]['repo']][dataset[0]['version']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker\n",
    "\n",
    "def image_exists(name):\n",
    "    # client = docker.from_env()\n",
    "    client = docker.DockerClient(base_url='unix:///home/lahiru-menik/.docker/desktop/docker.sock')\n",
    "    # print(client.images.list(), \"lll\")\n",
    "    try:\n",
    "        client.images.get(name)\n",
    "        return True\n",
    "    except docker.errors.ImageNotFound:\n",
    "        return False\n",
    "    except docker.errors.APIError as e:\n",
    "        print(f\"Docker API error: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(image_exists(\"django_4.1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Image: 'swe_py_3.9:latest', 'swe_py_4.1:latest'>, <Image: 'py_3.10:latest'>, <Image: 'swe-ubuntu-base:latest'>, <Image: 'hello-world:latest'>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = docker.DockerClient(base_url='unix:///home/lahiru-menik/.docker/desktop/docker.sock')\n",
    "images = client.images.list()\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Docker SDK sees the following images ===\n",
      "sha256:74cc54e27dc41bb10dc4b2226072d469509f2f22f1a3ce74f4a59661a1d44602\n",
      "Tags: ['hello-world:latest']\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "import docker\n",
    "\n",
    "client = docker.from_env()\n",
    "images = client.images.list()\n",
    "\n",
    "print(\"=== Docker SDK sees the following images ===\")\n",
    "for image in images:\n",
    "    print(image.id)\n",
    "    print(\"Tags:\", image.tags)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker SDK connected to: http+docker://localhost\n"
     ]
    }
   ],
   "source": [
    "import docker\n",
    "client = docker.from_env()\n",
    "print(\"Docker SDK connected to:\", client.api.base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_urlfield_clean_invalid (forms_tests.field_tests.test_urlfield.URLFieldTest)', 'test_urlfield_clean_not_required (forms_tests.field_tests.test_urlfield.URLFieldTest)']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Suppose this is your string (from dataset[0]['FAIL_TO_PASS'])\n",
    "fail_to_passs = '[\"test_urlfield_clean_invalid (forms_tests.field_tests.test_urlfield.URLFieldTest)\", \"test_urlfield_clean_not_required (forms_tests.field_tests.test_urlfield.URLFieldTest)\"]'\n",
    "\n",
    "# Convert the string to a real list\n",
    "fail_list = ast.literal_eval(fail_to_passs)\n",
    "\n",
    "print(fail_list)\n",
    "print(type(fail_list))  # should show <class 'list'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_urlfield_clean_invalid': 'forms_tests.field_tests.test_urlfield.URLFieldTest', 'test_urlfield_clean_not_required': 'forms_tests.field_tests.test_urlfield.URLFieldTest'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "fail_to_passs = dataset[0]['FAIL_TO_PASS']\n",
    "fail_to_passs = ast.literal_eval(fail_to_passs)\n",
    "\n",
    "\n",
    "fail_to_passs= {\n",
    "    re.sub(r'\\s*\\(.*\\)', '', item): re.search(r'\\((.*?)\\)', item).group(1)\n",
    "    for item in fail_to_passs\n",
    "}\n",
    "\n",
    "print(fail_to_passs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lahiru-menik/Documents/fyp/e19-4yp-Solve-Issues-In-Large-Code-Repositories/code/retriever/commit.py:147: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "django\n",
      "DiGraph with 27538 nodes and 40138 edges\n",
      "Checked out to 4fd3044ca0135da903a70dfb66992293f529ecf1\n",
      "DiGraph with 27538 nodes and 40138 edges\n",
      "version_exist_check: True\n",
      "Generated Dockerfile for django_4.1\n"
     ]
    }
   ],
   "source": [
    "from commit import update\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import pexpect\n",
    "import subprocess\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    commit_id = dataset[i]['base_commit']\n",
    "    fail_to_passs = dataset[i]['FAIL_TO_PASS']\n",
    "    fail_to_passs = ast.literal_eval(fail_to_passs)\n",
    "\n",
    "\n",
    "    fail_to_passs= {\n",
    "        re.sub(r'\\s*\\(.*\\)', '', item): re.search(r'\\((.*?)\\)', item).group(1)\n",
    "        for item in fail_to_passs\n",
    "    }\n",
    "\n",
    "    version = dataset[i]['version']\n",
    "    pass_to_pass = dataset[i]['PASS_TO_PASS']\n",
    "    name = dataset[i]['instance_id'].split(\"__\")[0]\n",
    "    # print(name)\n",
    "    update(name, commit_id)\n",
    "    bash = bash_session.BashSession()\n",
    "    bash.run_command(f\"cp -r {name} testbed\")\n",
    "    host_dir = os.path.abspath(\".\") + \"/testbed/\" + name\n",
    "    bash.logfile = open(f\"{name}_{version}_dockerbuild.log\", \"w\", encoding=\"utf-8\")\n",
    "    specs = MAP_REPO_VERSION_TO_SPECS_PY[dataset[i]['repo']][dataset[i]['version']]\n",
    "    container_dir = \"/testbed\"\n",
    "    version_exist_check = image_exists(f\"swe_py_{specs['python']}\")\n",
    "    print(\"version_exist_check:\", version_exist_check)\n",
    "    if not version_exist_check:\n",
    "        dockerfile_str_version = docker_build.generate_dockerfile_env(\n",
    "            python_version= specs[\"python\"],\n",
    "            \n",
    "        )    \n",
    "        tmpdir = tempfile.mkdtemp()\n",
    "        Path(f\"{tmpdir}/Dockerfile\").write_text(dockerfile_str_version)\n",
    "        print(f\"Generated Dockerfile for {name}_{specs['python']}\")\n",
    "        bash.run_command(f\"docker build -t swe_py_{specs['python']} {tmpdir}\")\n",
    "        # subprocess.run([\"docker\", \"build\", \"-t\", f\"swe_py_{specs['python']}\", tmpdir], check=True)\n",
    "    exist_check = image_exists(name + \"_\" + version)\n",
    "    # print(exist_check)\n",
    "    \n",
    "    # # exist_check = bash.run_command(f\"docker images | grep -q '^{name+'_'+version}' && echo yes || echo no\")\n",
    "    if not exist_check:\n",
    "    #     print(f\"Building image {name}_{version}...\")\n",
    "\n",
    "        if \"pre_install\" in specs:\n",
    "            pre_install_commands = \"\\n\".join(specs[\"pre_install\"])\n",
    "        else:\n",
    "            pre_install_commands = \"\"\n",
    "        dockerfile_str = docker_build.generate_dockerfile_packages(\n",
    "            python_version= specs[\"python\"],\n",
    "            pre_install= pre_install_commands,      \n",
    "        )\n",
    "        \n",
    "        tmpdir = tempfile.mkdtemp()\n",
    "        Path(f\"{tmpdir}/Dockerfile\").write_text(dockerfile_str)\n",
    "        print(f\"Generated Dockerfile for {name}_{version}\")\n",
    "        bash.run_command(f\"docker build -t {name}_{version} {tmpdir}\")\n",
    "    if \"pip_packages\" in specs:\n",
    "            pip_packages = \" \".join(specs[\"pip_packages\"])\n",
    "    else:\n",
    "            pip_packages = \"\"\n",
    "      \n",
    "    # bash.run_command(f\"docker run --mount type=bind,src={host_dir},dst={container_dir} -it {name}_{version} bash\")\n",
    "    # # if \"install\" in specs:\n",
    "    #     bash.run_command(specs[\"install\"])\n",
    "    # if \"packages\" in specs:\n",
    "    #     if not specs[\"packages\"].startswith(\"requiremen\"):\n",
    "    #         bash.run_command(f\"pip install {specs['packages']}\")\n",
    "    #     else:\n",
    "    #         if dataset[i]['repo'] in MAP_REPO_TO_REQS_PATHS:\n",
    "    #             requirements_path = MAP_REPO_TO_REQS_PATHS[dataset[i]['repo']]\n",
    "    #             requirements_path = \" \".join(requirements_path)\n",
    "    #         else:\n",
    "    #             requirements_path = specs['packages']\n",
    "    #         bash.run_command(f\"pip install -r {requirements_path}\")\n",
    "\n",
    "    # for test_cm in fail_to_passs.values():\n",
    "    #     print(f\"Running test: {test_cm}\")\n",
    "    #     print(bash.run_command(f\"{specs['test_cmd']} {test_cm}\"))\n",
    "    bash.close()\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentless",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
